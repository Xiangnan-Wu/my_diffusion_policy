#!/usr/bin/env python3
"""
å°†pickleæ ¼å¼çš„æ•°æ®è½¬æ¢ä¸ºæ›´ç¨³å®šçš„zarræ ¼å¼
è§£å†³NumPyç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
"""

import json
import pickle
import sys
import traceback
from pathlib import Path
from typing import Any, Dict

import numpy as np
import zarr


def create_numpy_compatibility_patch():
    """åˆ›å»ºNumPyå…¼å®¹æ€§è¡¥ä¸ï¼Œä¸´æ—¶è§£å†³åŠ è½½é—®é¢˜"""
    import sys

    # ä¿®è¡¥numpy.core.multiarrayç¼ºå¤±çš„ç±»å‹å±æ€§
    try:
        import numpy as np
        from numpy.core import multiarray

        missing_types = {
            "flexible": np.flexible,
            "number": np.number,
            "integer": np.integer,
            "signedinteger": np.signedinteger,
            "unsignedinteger": np.unsignedinteger,
            "inexact": np.inexact,
            "floating": np.floating,
            "complexfloating": np.complexfloating,
            "character": np.character,
            "bool_": np.bool_,
            "void": np.void,
        }

        for attr_name, attr_value in missing_types.items():
            if not hasattr(multiarray, attr_name):
                setattr(multiarray, attr_name, attr_value)

    except Exception as e:
        print(f"è­¦å‘Šï¼šNumPyå…¼å®¹æ€§è¡¥ä¸å¤±è´¥: {e}")

    # åˆ›å»ºnumpy._coreæ˜ å°„
    if "numpy._core" not in sys.modules:
        try:
            import numpy.core as _core

            sys.modules["numpy._core"] = _core

            # æ˜ å°„å­æ¨¡å—
            if hasattr(_core, "multiarray"):
                sys.modules["numpy._core.multiarray"] = _core.multiarray
            if hasattr(_core, "numeric"):
                sys.modules["numpy._core.numeric"] = _core.numeric
            if hasattr(_core, "umath"):
                sys.modules["numpy._core.umath"] = _core.umath

        except ImportError:
            pass


def safe_load_pickle(filepath: str) -> Any:
    """å®‰å…¨åŠ è½½pickleæ–‡ä»¶"""
    try:
        with open(filepath, "rb") as f:
            return pickle.load(f, fix_imports=True, encoding="latin1")
    except Exception as e:
        print(f"æ ‡å‡†åŠ è½½å¤±è´¥ {filepath}: {e}")
        try:
            with open(filepath, "rb") as f:
                return pickle.load(f)
        except Exception as e2:
            print(f"å¤‡ç”¨åŠ è½½ä¹Ÿå¤±è´¥ {filepath}: {e2}")
            return None


def convert_pickle_to_zarr(input_dir: str, output_file: str) -> bool:
    """
    å°†pickleæ ¼å¼çš„æ•°æ®è½¬æ¢ä¸ºzarræ ¼å¼(é€‚é…Diffusion Policy)

    Args:
        input_dir: è¾“å…¥ç›®å½•è·¯å¾„
        output_file: è¾“å‡ºzarræ–‡ä»¶è·¯å¾„

    Returns:
        è½¬æ¢æ˜¯å¦æˆåŠŸ
    """
    try:
        print(f"å¼€å§‹è½¬æ¢: {input_dir} -> {output_file}")

        # 1. æ‰«ææ•°æ®ç»“æ„
        input_path = Path(input_dir)
        if not input_path.exists():
            print(f"é”™è¯¯ï¼šè¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
            return False

        episodes = sorted(
            [d for d in input_path.iterdir() if d.is_dir()],
            key=lambda x: int(x.name.split("_")[-1]),
        )

        if not episodes:
            print("é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°episodeç›®å½•")
            return False

        # 2. åˆ†ææ•°æ®ç»“æ„
        first_episode = episodes[0]
        data_types = [d.name for d in first_episode.iterdir() if d.is_dir()]
        # ä¸æ’é™¤ä»»ä½•æ•°æ®ç±»å‹ï¼Œå…¨éƒ¨è½¬æ¢
        print(f"åŸå§‹æ•°æ®ç±»å‹: {data_types}")

        print(f"æ‰¾åˆ° {len(episodes)} ä¸ªepisodes")
        print(f"æ•°æ®ç±»å‹: {data_types}")

        # 3. åˆ›å»ºzarrå­˜å‚¨
        store = zarr.DirectoryStore(output_file)
        root = zarr.group(store=store, overwrite=True)

        # åˆ›å»ºdataå’Œmetaå­ç»„
        data_group = root.create_group("data")
        meta_group = root.create_group("meta")

        # 4. è½¬æ¢æ¯ç§æ•°æ®ç±»å‹
        converted_data = {}
        episode_lengths = []

        for episode_idx, episode_dir in enumerate(episodes):
            print(f"å¤„ç†episode {episode_idx}/{len(episodes)}: {episode_dir.name}")

            episode_data = {}

            for data_type in data_types:
                if data_type == "gripper_states":
                    continue
                elif data_type == "poses":
                    data_path = episode_dir / data_type
                    if not data_path.exists():
                        continue
                    gripper_path = episode_dir / "gripper_states"
                    if not gripper_path.exists():
                        continue
                    files = sorted(data_path.glob("*.pkl"), key=lambda x: int(x.stem))
                    gripper_files = sorted(
                        gripper_path.glob("*.pkl"), key=lambda x: int(x.stem)
                    )

                    # åŠ è½½æ‰€æœ‰poseå’Œgripperæ•°æ®
                    all_pose_data = []
                    for i in range(len(files)):
                        file_path = files[i]
                        gripper_file_path = gripper_files[i]
                        pose_data = safe_load_pickle(str(file_path))
                        gripper_data = safe_load_pickle(str(gripper_file_path))
                        full_pose = np.concatenate(
                            [pose_data, [float(gripper_data)]], axis=0
                        )
                        all_pose_data.append(full_pose)

                    if len(all_pose_data) > 1:
                        # robot_eef_pose: tæ—¶åˆ»çš„pose (é™¤äº†æœ€åä¸€ä¸ªæ—¶é—´æ­¥)
                        robot_eef_pose = np.stack(all_pose_data[:-1])
                        # action: t+1æ—¶åˆ»çš„poseä½œä¸ºtæ—¶åˆ»çš„action (é™¤äº†ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥)
                        action = np.stack(all_pose_data[1:])

                        episode_data["robot_eef_pose"] = robot_eef_pose
                        episode_data["action"] = action

                        # è®¾ç½®æ ‡å¿—ï¼Œè¡¨ç¤ºéœ€è¦å¯¹é½å…¶ä»–æ•°æ®
                        pose_length = len(robot_eef_pose)  # æ–°çš„æ—¶é—´æ­¥é•¿åº¦
                else:
                    data_path = episode_dir / data_type
                    if not data_path.exists():
                        continue

                    # åŠ è½½è¯¥ç±»å‹çš„æ‰€æœ‰æ–‡ä»¶
                    files = sorted(data_path.glob("*.pkl"), key=lambda x: int(x.stem))

                    type_data = []
                    for file_path in files:
                        data = safe_load_pickle(str(file_path))
                        if data is not None:
                            type_data.append(data)
                        else:
                            print(f"è·³è¿‡æ–‡ä»¶: {file_path}")

                    if type_data:
                        # è½¬æ¢ä¸ºnumpyæ•°ç»„
                        if isinstance(type_data[0], np.ndarray):
                            episode_data[data_type] = np.stack(type_data)
                        else:
                            episode_data[data_type] = np.array(type_data)

            # å¦‚æœå¤„ç†äº†posesæ•°æ®ï¼Œéœ€è¦å¯¹é½å…¶ä»–æ•°æ®çš„é•¿åº¦
            if "robot_eef_pose" in episode_data and "action" in episode_data:
                target_length = len(episode_data["robot_eef_pose"])

                # å¯¹é½å…¶ä»–æ•°æ®ç±»å‹ï¼Œä¿æŒä¸robot_eef_poseç›¸åŒçš„æ—¶é—´æ­¥
                for key in list(episode_data.keys()):
                    if key not in ["robot_eef_pose", "action"]:
                        data = episode_data[key]
                        if len(data) > target_length:
                            # æˆªå–å‰target_lengthä¸ªæ—¶é—´æ­¥ï¼Œä¸robot_eef_poseå¯¹é½
                            episode_data[key] = data[:target_length]
                        elif len(data) < target_length:
                            print(
                                f"è­¦å‘Š: {key} æ•°æ®é•¿åº¦ ({len(data)}) å°äºç›®æ ‡é•¿åº¦ ({target_length})"
                            )

            # è®°å½•episodeé•¿åº¦
            if episode_data:
                episode_length = len(list(episode_data.values())[0])
                episode_lengths.append(episode_length)

                # æ·»åŠ åˆ°æ€»æ•°æ®ä¸­
                for data_type, data_array in episode_data.items():
                    if data_type not in converted_data:
                        converted_data[data_type] = []
                    converted_data[data_type].append(data_array)

        # 5. åˆå¹¶æ‰€æœ‰episodesçš„æ•°æ®
        print("åˆå¹¶æ•°æ®...")
        for data_type, episode_list in converted_data.items():
            if episode_list:
                # æ‹¼æ¥æ‰€æœ‰episodes
                combined_data = np.concatenate(episode_list, axis=0)

                # ä¿å­˜åˆ°dataç»„
                print(f"ä¿å­˜ {data_type}: {combined_data.shape}")
                data_group.create_dataset(
                    data_type,
                    data=combined_data,
                    chunks=combined_data.shape,  # è‡ªåŠ¨é€‰æ‹©chunkå¤§å°
                    compression=None,  # ä½¿ç”¨å‹ç¼©
                    dtype=combined_data.dtype,
                )

        # 6. ä¿å­˜å…ƒæ•°æ®åˆ°metaç»„
        episode_lengths_array = np.array(episode_lengths, dtype=np.int64)
        episode_ends_array = np.cumsum(episode_lengths_array)

        # ä¿å­˜å„ç§å…ƒæ•°æ®
        meta_group.create_dataset("episode_lengths", data=episode_lengths_array)
        meta_group.create_dataset("episode_ends", data=episode_ends_array)
        meta_group.create_dataset(
            "n_episodes", data=np.array(len(episodes), dtype=np.int64)
        )
        meta_group.create_dataset(
            "total_steps", data=np.array(sum(episode_lengths), dtype=np.int64)
        )

        # ä¿å­˜æ•°æ®ç±»å‹åˆ—è¡¨ä¸ºå±æ€§
        meta_group.attrs["data_types"] = list(converted_data.keys())

        # ä¸ºå…¼å®¹æ€§ï¼Œä¹Ÿåœ¨rootçº§åˆ«ä¿å­˜ä¸€ä»½å…ƒæ•°æ®
        metadata = {
            "n_episodes": len(episodes),
            "episode_lengths": episode_lengths,
            "episode_ends": episode_ends_array.tolist(),
            "data_types": list(converted_data.keys()),
            "total_steps": sum(episode_lengths),
        }
        root.attrs["metadata"] = json.dumps(metadata)

        print("âœ… è½¬æ¢å®Œæˆï¼")
        print(f"æ€»episodes: {metadata['n_episodes']}")
        print(f"æ€»æ­¥æ•°: {metadata['total_steps']}")
        print(f"æ•°æ®ç±»å‹: {metadata['data_types']}")
        print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")

        return True

    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def load_zarr_data(zarr_file: str) -> Dict[str, Any]:
    """
    åŠ è½½zarræ ¼å¼çš„æ•°æ®

    Args:
        zarr_file: zarræ–‡ä»¶è·¯å¾„

    Returns:
        åŒ…å«æ•°æ®å’Œå…ƒæ•°æ®çš„å­—å…¸
    """
    try:
        store = zarr.DirectoryStore(zarr_file)
        root = zarr.group(store=store, mode="r")

        # åŠ è½½å…ƒæ•°æ®
        metadata = json.loads(root.attrs["metadata"])

        # åŠ è½½æ•°æ®
        data = {}
        for key in root.keys():
            data[key] = root[key]

        return {"data": data, "metadata": metadata}

    except Exception as e:
        print(f"åŠ è½½zarræ•°æ®å¤±è´¥: {e}")
        return None


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 3:
        print("ç”¨æ³•: python convert_pickle_to_zarr.py <input_dir> <output_zarr>")
        print(
            "ç¤ºä¾‹: python convert_pickle_to_zarr.py ./data/continuous_data_dev ./data/continuous_data_dev.zarr"
        )
        return

    input_dir = sys.argv[1]
    output_file = sys.argv[2]

    # åº”ç”¨å…¼å®¹æ€§è¡¥ä¸
    create_numpy_compatibility_patch()

    # æ‰§è¡Œè½¬æ¢
    success = convert_pickle_to_zarr(input_dir, output_file)

    if success:
        print("\nğŸ‰ æ•°æ®è½¬æ¢æˆåŠŸï¼")
        print("ç°åœ¨ä½ å¯ä»¥ä½¿ç”¨zarræ ¼å¼çš„æ•°æ®ï¼Œé¿å…pickleå…¼å®¹æ€§é—®é¢˜")
        print("åŠ è½½ç¤ºä¾‹:")
        print("import zarr")
        print(f"data = zarr.open('{output_file}', mode='r')")
    else:
        print("\nâŒ æ•°æ®è½¬æ¢å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()
